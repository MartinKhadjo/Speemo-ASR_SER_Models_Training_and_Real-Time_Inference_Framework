[DIST] MASTER_ADDR=n23g0007 MASTER_PORT=29500 NNODES=1 GPUS_PER_NODE=1
[PRE] Skipping all preprocessing (skip_preprocessing=on or all per-step toggles are 'on')
[TRAIN][ASR] Starting ASR phase ...
[Setup] Using device: cuda
[Setup] no_cuda flag set to: False
[ASR] Loading model + processor‚Ä¶
[ASR] Starting from pretrained: models/pretrained/en
üõ†Ô∏è  Debug collator output shapes: {'input_values': torch.Size([4, 75264]), 'attention_mask': torch.Size([4, 75264]), 'labels': torch.Size([4, 93])}
[ASR] Starting training with HuggingFace Trainer‚Ä¶
n23g0007:1087995:1087995 [0] NCCL INFO Bootstrap : Using ib0:134.61.46.195<0>
n23g0007:1087995:1087995 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
n23g0007:1087995:1087995 [0] NCCL INFO cudaDriverVersion 12080
NCCL version 2.20.5+cuda12.4
n23g0007:1087995:1088076 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:134.61.46.195<0>
n23g0007:1087995:1088076 [0] NCCL INFO Using non-device net plugin version 0
n23g0007:1087995:1088076 [0] NCCL INFO Using network IB
n23g0007:1087995:1088076 [0] NCCL INFO DMA-BUF is available on GPU device 0
n23g0007:1087995:1088076 [0] NCCL INFO comm 0x55e2de96f710 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x35465410bd3e0fc1 - Init START
n23g0007:1087995:1088076 [0] NCCL INFO Setting affinity for GPU 0 to 15
n23g0007:1087995:1088076 [0] NCCL INFO comm 0x55e2de96f710 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 00/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 01/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 02/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 03/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 04/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 05/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 06/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 07/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 08/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 09/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 10/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 11/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 12/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 13/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 14/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 15/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 16/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 17/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 18/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 19/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 20/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 21/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 22/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 23/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 24/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 25/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 26/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 27/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 28/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 29/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 30/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Channel 31/32 :    0
n23g0007:1087995:1088076 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
n23g0007:1087995:1088076 [0] NCCL INFO P2P Chunksize set to 131072
n23g0007:1087995:1088076 [0] NCCL INFO Connected all rings
n23g0007:1087995:1088076 [0] NCCL INFO Connected all trees
n23g0007:1087995:1088076 [0] NCCL INFO 32 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
n23g0007:1087995:1088076 [0] NCCL INFO comm 0x55e2de96f710 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1b000 commId 0x35465410bd3e0fc1 - Init COMPLETE
{'loss': 2.1461, 'grad_norm': 2.406790256500244, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.6779, 'grad_norm': 3.5908896923065186, 'learning_rate': 2.450612653163291e-07, 'epoch': 0.03}
{'loss': 1.6984, 'grad_norm': 2.500959873199463, 'learning_rate': 4.951237809452364e-07, 'epoch': 0.05}
{'loss': 1.6533, 'grad_norm': 2.0225412845611572, 'learning_rate': 7.451862965741435e-07, 'epoch': 0.08}
{'loss': 1.5812, 'grad_norm': 1.31912362575531, 'learning_rate': 9.952488122030508e-07, 'epoch': 0.1}
{'loss': 1.553, 'grad_norm': 2.8253073692321777, 'learning_rate': 1.2428107026756691e-06, 'epoch': 0.13}
{'loss': 1.5399, 'grad_norm': 4.786060333251953, 'learning_rate': 1.4928732183045763e-06, 'epoch': 0.15}
{'loss': 1.5693, 'grad_norm': 3.1286065578460693, 'learning_rate': 1.7429357339334835e-06, 'epoch': 0.18}
{'loss': 1.6625, 'grad_norm': 2.0658998489379883, 'learning_rate': 1.9929982495623906e-06, 'epoch': 0.2}
{'loss': 1.6023, 'grad_norm': 1.9215949773788452, 'learning_rate': 2.243060765191298e-06, 'epoch': 0.23}
{'loss': 1.5872, 'grad_norm': 3.4128894805908203, 'learning_rate': 2.4931232808202054e-06, 'epoch': 0.25}
{'loss': 1.625, 'grad_norm': 3.258375406265259, 'learning_rate': 2.7431857964491125e-06, 'epoch': 0.28}
{'loss': 1.4983, 'grad_norm': 2.485567092895508, 'learning_rate': 2.9932483120780197e-06, 'epoch': 0.3}
{'loss': 1.473, 'grad_norm': 9.780546188354492, 'learning_rate': 3.243310827706927e-06, 'epoch': 0.33}
{'loss': 1.4191, 'grad_norm': 2.7437000274658203, 'learning_rate': 3.493373343335834e-06, 'epoch': 0.35}
{'loss': 1.3843, 'grad_norm': 2.8548920154571533, 'learning_rate': 3.743435858964741e-06, 'epoch': 0.38}
{'loss': 1.3821, 'grad_norm': 2.193880796432495, 'learning_rate': 3.993498374593649e-06, 'epoch': 0.4}
{'loss': 1.2405, 'grad_norm': 4.796090602874756, 'learning_rate': 4.243560890222556e-06, 'epoch': 0.43}
{'loss': 1.2472, 'grad_norm': 3.2582767009735107, 'learning_rate': 4.4936234058514635e-06, 'epoch': 0.45}
