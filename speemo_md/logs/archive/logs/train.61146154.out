[PRE] Skipping all preprocessing (skip_preprocessing=on or all per-step toggles are 'on')
[TRAIN][ASR] Starting ASR phase ...
Resolved Slurm tools at startup: ('/usr/bin/sbatch', '/usr/bin/squeue', '/usr/bin/sacct') HPCWORK= None
[Flask] Using device for inference: cuda
Running training: torchrun --nproc_per_node=1 --rdzv_id=training --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1:29500 /workspace/src/train.py --device=cuda --phase=asr --asr_learning_rate=1e-05 --asr_batch_size=4 --asr_epochs=10 --asr_patience=2 --asr_checkpoint=models/checkpoints/Speemo_Medium_Dataset_HPC_GPU_based_Train_102_asr --asr_lang=en --ser_learning_rate=1e-05 --ser_batch_size=4 --ser_epochs=10 --ser_dropout=0.3 --ser_patience=2 --ser_checkpoint=models/checkpoints/Speemo_Medium_Dataset_HPC_GPU_based_Train_102_ser --ser_lang=en
[Setup] Using device: cuda
[Setup] no_cuda flag set to: False
[ASR] Loading model + processor‚Ä¶
[ASR] Starting from pretrained: models/pretrained/en
üõ†Ô∏è  Debug collator output shapes: {'input_values': torch.Size([4, 75264]), 'attention_mask': torch.Size([4, 75264]), 'labels': torch.Size([4, 93])}
[ASR] Starting training with HuggingFace Trainer‚Ä¶
w23g0007:910253:910253 [0] NCCL INFO Bootstrap : Using ib0:134.61.46.236<0>
w23g0007:910253:910253 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
w23g0007:910253:910253 [0] NCCL INFO cudaDriverVersion 12080
NCCL version 2.20.5+cuda12.4
w23g0007:910253:910342 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:134.61.46.236<0>
w23g0007:910253:910342 [0] NCCL INFO Using non-device net plugin version 0
w23g0007:910253:910342 [0] NCCL INFO Using network IB
w23g0007:910253:910342 [0] NCCL INFO DMA-BUF is available on GPU device 0
w23g0007:910253:910342 [0] NCCL INFO comm 0x562521cd4f30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 2c000 commId 0x37b88879eeb9a0f0 - Init START
w23g0007:910253:910342 [0] NCCL INFO Setting affinity for GPU 0 to 1e000000
w23g0007:910253:910342 [0] NCCL INFO comm 0x562521cd4f30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
w23g0007:910253:910342 [0] NCCL INFO Channel 00/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 01/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 02/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 03/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 04/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 05/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 06/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 07/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 08/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 09/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 10/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 11/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 12/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 13/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 14/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 15/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 16/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 17/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 18/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 19/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 20/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 21/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 22/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 23/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 24/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 25/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 26/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 27/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 28/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 29/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 30/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Channel 31/32 :    0
w23g0007:910253:910342 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
w23g0007:910253:910342 [0] NCCL INFO P2P Chunksize set to 131072
w23g0007:910253:910342 [0] NCCL INFO Connected all rings
w23g0007:910253:910342 [0] NCCL INFO Connected all trees
w23g0007:910253:910342 [0] NCCL INFO 32 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w23g0007:910253:910342 [0] NCCL INFO comm 0x562521cd4f30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 2c000 commId 0x37b88879eeb9a0f0 - Init COMPLETE
{'loss': 2.1461, 'grad_norm': 2.156780481338501, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.6778, 'grad_norm': 3.437028169631958, 'learning_rate': 2.475618904726182e-07, 'epoch': 0.03}
{'loss': 1.6989, 'grad_norm': 2.2164433002471924, 'learning_rate': 4.951237809452364e-07, 'epoch': 0.05}
{'loss': 1.6546, 'grad_norm': 1.6894323825836182, 'learning_rate': 7.451862965741435e-07, 'epoch': 0.08}
{'loss': 1.5837, 'grad_norm': 1.139283537864685, 'learning_rate': 9.952488122030508e-07, 'epoch': 0.1}
{'loss': 1.5581, 'grad_norm': 2.6956958770751953, 'learning_rate': 1.2453113278319582e-06, 'epoch': 0.13}
{'loss': 1.5466, 'grad_norm': 4.952066898345947, 'learning_rate': 1.4953738434608653e-06, 'epoch': 0.15}
{'loss': 1.579, 'grad_norm': 2.7998783588409424, 'learning_rate': 1.7454363590897727e-06, 'epoch': 0.18}
{'loss': 1.6752, 'grad_norm': 2.0313849449157715, 'learning_rate': 1.99549887471868e-06, 'epoch': 0.2}
{'loss': 1.6145, 'grad_norm': 1.631074070930481, 'learning_rate': 2.245561390347587e-06, 'epoch': 0.23}
{'loss': 1.6003, 'grad_norm': 3.106835126876831, 'learning_rate': 2.4956239059764944e-06, 'epoch': 0.25}
{'loss': 1.6416, 'grad_norm': 3.2526371479034424, 'learning_rate': 2.7431857964491125e-06, 'epoch': 0.28}
{'loss': 1.5122, 'grad_norm': 2.354498863220215, 'learning_rate': 2.9932483120780197e-06, 'epoch': 0.3}
{'loss': 1.4871, 'grad_norm': 10.365283012390137, 'learning_rate': 3.243310827706927e-06, 'epoch': 0.33}
{'loss': 1.4331, 'grad_norm': 2.665346622467041, 'learning_rate': 3.493373343335834e-06, 'epoch': 0.35}
{'loss': 1.395, 'grad_norm': 2.590588092803955, 'learning_rate': 3.743435858964741e-06, 'epoch': 0.38}
{'loss': 1.3917, 'grad_norm': 2.2983665466308594, 'learning_rate': 3.993498374593649e-06, 'epoch': 0.4}
{'loss': 1.2502, 'grad_norm': 4.690283298492432, 'learning_rate': 4.241060265066267e-06, 'epoch': 0.43}
{'loss': 1.2579, 'grad_norm': 3.648285388946533, 'learning_rate': 4.491122780695174e-06, 'epoch': 0.45}
{'loss': 1.3211, 'grad_norm': 8.682766914367676, 'learning_rate': 4.741185296324081e-06, 'epoch': 0.48}
{'loss': 1.309, 'grad_norm': 2.84134840965271, 'learning_rate': 4.991247811952989e-06, 'epoch': 0.5}
{'loss': 1.214, 'grad_norm': 5.314402103424072, 'learning_rate': 5.2413103275818955e-06, 'epoch': 0.53}
{'loss': 1.2363, 'grad_norm': 4.43014669418335, 'learning_rate': 5.491372843210802e-06, 'epoch': 0.55}
{'loss': 1.1638, 'grad_norm': 3.0469987392425537, 'learning_rate': 5.741435358839711e-06, 'epoch': 0.58}
{'loss': 1.2387, 'grad_norm': 3.0220234394073486, 'learning_rate': 5.988997249312328e-06, 'epoch': 0.6}
{'loss': 1.1908, 'grad_norm': 3.011845350265503, 'learning_rate': 6.239059764941235e-06, 'epoch': 0.63}
{'loss': 1.2314, 'grad_norm': 3.3546805381774902, 'learning_rate': 6.489122280570143e-06, 'epoch': 0.65}
{'loss': 1.186, 'grad_norm': 5.586483955383301, 'learning_rate': 6.739184796199051e-06, 'epoch': 0.68}
{'loss': 1.2265, 'grad_norm': 3.848418712615967, 'learning_rate': 6.989247311827958e-06, 'epoch': 0.7}
{'loss': 1.1912, 'grad_norm': 2.50738525390625, 'learning_rate': 7.2393098274568655e-06, 'epoch': 0.73}
{'loss': 1.1176, 'grad_norm': 4.745069980621338, 'learning_rate': 7.489372343085772e-06, 'epoch': 0.75}
{'loss': 1.1415, 'grad_norm': 5.13629674911499, 'learning_rate': 7.739434858714679e-06, 'epoch': 0.78}
{'loss': 1.1105, 'grad_norm': 2.1248230934143066, 'learning_rate': 7.989497374343587e-06, 'epoch': 0.8}
{'loss': 1.175, 'grad_norm': 4.349660873413086, 'learning_rate': 8.239559889972494e-06, 'epoch': 0.83}
{'loss': 1.1426, 'grad_norm': 3.6151161193847656, 'learning_rate': 8.489622405601401e-06, 'epoch': 0.85}
{'loss': 1.0644, 'grad_norm': 3.285630226135254, 'learning_rate': 8.739684921230308e-06, 'epoch': 0.88}
{'loss': 1.0943, 'grad_norm': 14.200984001159668, 'learning_rate': 8.989747436859216e-06, 'epoch': 0.9}
{'loss': 1.0575, 'grad_norm': 3.3307905197143555, 'learning_rate': 9.239809952488123e-06, 'epoch': 0.93}
{'loss': 1.1431, 'grad_norm': 4.707984924316406, 'learning_rate': 9.48987246811703e-06, 'epoch': 0.95}
{'loss': 0.9768, 'grad_norm': 9.552072525024414, 'learning_rate': 9.739934983745938e-06, 'epoch': 0.98}
{'eval_loss': 0.8338858485221863, 'eval_wer': 0.4086635772877384, 'eval_runtime': 1424.2477, 'eval_samples_per_second': 5.617, 'eval_steps_per_second': 5.617, 'epoch': 1.0}
{'loss': 1.0002, 'grad_norm': 3.8027660846710205, 'learning_rate': 9.989997499374845e-06, 'epoch': 1.0}
{'loss': 1.0704, 'grad_norm': 3.0455379486083984, 'learning_rate': 1e-05, 'epoch': 1.03}
{'loss': 0.9032, 'grad_norm': 17.42387580871582, 'learning_rate': 1e-05, 'epoch': 1.05}
{'loss': 1.0502, 'grad_norm': 7.028504371643066, 'learning_rate': 1e-05, 'epoch': 1.08}
{'loss': 1.0384, 'grad_norm': 1.8091323375701904, 'learning_rate': 1e-05, 'epoch': 1.1}
{'loss': 0.9877, 'grad_norm': 4.669622898101807, 'learning_rate': 1e-05, 'epoch': 1.13}
{'loss': 1.0533, 'grad_norm': 6.491848945617676, 'learning_rate': 1e-05, 'epoch': 1.15}
{'loss': 0.9429, 'grad_norm': 2.714482545852661, 'learning_rate': 1e-05, 'epoch': 1.18}
{'loss': 0.9916, 'grad_norm': 8.152520179748535, 'learning_rate': 1e-05, 'epoch': 1.2}
{'loss': 0.9993, 'grad_norm': 3.3127191066741943, 'learning_rate': 1e-05, 'epoch': 1.23}
{'loss': 0.9432, 'grad_norm': 7.097261428833008, 'learning_rate': 1e-05, 'epoch': 1.25}
{'loss': 1.0057, 'grad_norm': 3.8683390617370605, 'learning_rate': 1e-05, 'epoch': 1.28}
{'loss': 0.9247, 'grad_norm': 2.8099875450134277, 'learning_rate': 1e-05, 'epoch': 1.3}
{'loss': 1.0213, 'grad_norm': 5.4069366455078125, 'learning_rate': 1e-05, 'epoch': 1.33}
{'loss': 0.9136, 'grad_norm': 3.3899552822113037, 'learning_rate': 1e-05, 'epoch': 1.35}
{'loss': 0.974, 'grad_norm': 25.79150390625, 'learning_rate': 1e-05, 'epoch': 1.38}
{'loss': 0.9944, 'grad_norm': 21.564908981323242, 'learning_rate': 1e-05, 'epoch': 1.4}
{'loss': 0.9478, 'grad_norm': 1.9700201749801636, 'learning_rate': 1e-05, 'epoch': 1.43}
{'loss': 0.9879, 'grad_norm': 4.986824035644531, 'learning_rate': 1e-05, 'epoch': 1.45}
{'loss': 1.009, 'grad_norm': 1.1584892272949219, 'learning_rate': 1e-05, 'epoch': 1.48}
{'loss': 0.9493, 'grad_norm': 6.037277698516846, 'learning_rate': 1e-05, 'epoch': 1.5}
{'loss': 0.9781, 'grad_norm': 2.4862215518951416, 'learning_rate': 1e-05, 'epoch': 1.53}
{'loss': 0.8871, 'grad_norm': 6.125113010406494, 'learning_rate': 1e-05, 'epoch': 1.55}
{'loss': 1.2967, 'grad_norm': 5.222502708435059, 'learning_rate': 1e-05, 'epoch': 1.58}
{'loss': 0.9608, 'grad_norm': 2.754031181335449, 'learning_rate': 1e-05, 'epoch': 1.6}
{'loss': 0.8865, 'grad_norm': 5.849959850311279, 'learning_rate': 1e-05, 'epoch': 1.63}
{'loss': 0.9945, 'grad_norm': 7.854594707489014, 'learning_rate': 1e-05, 'epoch': 1.65}
{'loss': 0.9873, 'grad_norm': 6.791136741638184, 'learning_rate': 1e-05, 'epoch': 1.68}
{'loss': 0.9364, 'grad_norm': 4.1118292808532715, 'learning_rate': 1e-05, 'epoch': 1.7}
{'loss': 1.0313, 'grad_norm': 4.057602882385254, 'learning_rate': 1e-05, 'epoch': 1.73}
{'loss': 0.9724, 'grad_norm': 2.8421077728271484, 'learning_rate': 1e-05, 'epoch': 1.75}
{'loss': 1.0074, 'grad_norm': 11.917032241821289, 'learning_rate': 1e-05, 'epoch': 1.78}
{'loss': 0.9103, 'grad_norm': 8.382984161376953, 'learning_rate': 1e-05, 'epoch': 1.8}
{'loss': 0.9331, 'grad_norm': 8.970947265625, 'learning_rate': 1e-05, 'epoch': 1.83}
{'loss': 1.0367, 'grad_norm': 7.868522644042969, 'learning_rate': 1e-05, 'epoch': 1.85}
{'loss': 0.8856, 'grad_norm': 13.687456130981445, 'learning_rate': 1e-05, 'epoch': 1.88}
{'loss': 1.0504, 'grad_norm': 3.2678468227386475, 'learning_rate': 1e-05, 'epoch': 1.9}
{'loss': 0.9714, 'grad_norm': 5.006137371063232, 'learning_rate': 1e-05, 'epoch': 1.93}
{'loss': 0.973, 'grad_norm': 9.71273136138916, 'learning_rate': 1e-05, 'epoch': 1.95}
{'loss': 0.9216, 'grad_norm': 9.600671768188477, 'learning_rate': 1e-05, 'epoch': 1.98}
